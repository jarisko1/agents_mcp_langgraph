{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37f0dad",
   "metadata": {},
   "source": [
    "# Test notebook\n",
    "- Instructions: https://huggingface.co/learn/agents-course/unit4/hands-on\n",
    "- Leaderboard and codes: https://huggingface.co/spaces/agents-course/Students_leaderboard\n",
    "    - 1 good example: https://huggingface.co/spaces/cellerson/AgentCourseFinalProject/tree/main\n",
    "- GitHub models: https://github.com/marketplace?type=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd8c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import inspect\n",
    "import typing\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_url = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "questions_url = f\"{api_url}/questions\"\n",
    "random_question_url = f\"{api_url}/random-question\"\n",
    "files_url = f\"{api_url}/files\"\n",
    "submit_url = f\"{api_url}/submit\"\n",
    "\n",
    "def get_question() -> typing.Tuple[str, str, bytes | None, str | None]:\n",
    "    \"\"\"\n",
    "    Get a random question from the API.\n",
    "    Returns\n",
    "        - question text\n",
    "        - task id\n",
    "        - binary file (if any)\n",
    "        - file name (if any)\n",
    "    \"\"\"\n",
    "\n",
    "    question_data = requests.get(questions_url, timeout=15)\n",
    "    question_data.raise_for_status()\n",
    "    questions_json = question_data.json()[0]\n",
    "\n",
    "    question = questions_json[\"question\"]\n",
    "    task_id = questions_json[\"task_id\"]\n",
    "    file_content = None\n",
    "\n",
    "    file_name = questions_json.get(\"file_name\", None)\n",
    "\n",
    "    if file_name:\n",
    "        file_url = f\"{files_url}/{task_id}\"\n",
    "        response = requests.get(file_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        # with open(os.path.join(\"tmp\", file_name), \"wb\") as f:\n",
    "        #     f.write(response.content)\n",
    "        file_content = response.content\n",
    "\n",
    "    return question, task_id, file_content, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
      "Response:  Between 2000 and 2009, Mercedes Sosa released eight studio albums. Here is a list of those albums:\n",
      "\n",
      "1. **Mujeres Argentinas** (2000)\n",
      "2. **Cantora** (2001)\n",
      "3. **30 Años de Vida de Mercedes Sosa** (2004)\n",
      "4. **Y cómo es él** (2005)\n",
      "5. **Canta por los ausentes** (2006) – Although primarily distributed as a promotion album, it has significant content.\n",
      "6. **Alta fidelidad** (2007)\n",
      "7. **Rodolfo** (2008) – A collaboration with Joan Manuel Serrat.\n",
      "8. **Cantora 2** (2009)\n",
      "\n",
      "Please note that the released albums can vary in definition based on official releases versus compilations or promotional distributions, but these are the mainstream studio albums released within that time frame.\n"
     ]
    }
   ],
   "source": [
    "question, task_id, file_content, file_name = get_question()\n",
    "\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"microsoft/Phi-4\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(question),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c639857",
   "metadata": {},
   "source": [
    "# Agent\n",
    "## Tools:\n",
    "- Text from picture\n",
    "- Text from CSV\n",
    "- Text from XLSX\n",
    "- Internet search\n",
    "## Ideas:\n",
    "- Add another agent that will validate the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f45e6",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3a5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from typing import TypedDict, List, Dict, Annotated, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langchain_community.tools import Tool, DuckDuckGoSearchRun\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f846e0e",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba73e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(query: str) -> str:\n",
    "#     \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "#     results = bm25_retriever.invoke(query)\n",
    "#     if results:\n",
    "#         return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "#     else:\n",
    "#         return \"No matching guest information found.\"\n",
    "\n",
    "# guest_info_tool = Tool(\n",
    "#     name=\"guest_info_retriever\",\n",
    "#     func=extract_text,\n",
    "#     description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    "# )\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "# You can create the tool to pass to an agent\n",
    "python_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    search_tool,\n",
    "    python_tool,\n",
    "]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e6137",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef7b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskState(TypedDict):\n",
    "\n",
    "    # Input data\n",
    "    question: str\n",
    "    task_id: str\n",
    "    file_name: Optional[str]\n",
    "    file_content: Optional[bytes]\n",
    "\n",
    "    # Output data\n",
    "    answer: str\n",
    "    final_answer_reached: bool\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "def assistant(state: TaskState):\n",
    "\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"final_answer_reached\": True,\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3aec6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "task_graph = StateGraph(TaskState)\n",
    "\n",
    "# Add nodes\n",
    "task_graph.add_node(\"assistant\", assistant)\n",
    "task_graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges\n",
    "task_graph.add_edge(START, \"assistant\")\n",
    "task_graph.add_conditional_edges(\"assistant\", tools_condition)\n",
    "task_graph.add_edge(\"tools\", \"assistant\")\n",
    "# task_graph.add_edge(\"assistant\", END)\n",
    "\n",
    "compiled_graph = task_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30d30664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50e+00\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"What is the latest GPT model from OpenAI?\n",
    "Output only the numbers from the model with exponential notation with power -2.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI assistant anwering complex questions.\n",
    "Think step by step and answer following question.\n",
    "Always output only the answer formatted as specified in the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "task_answer = compiled_graph.invoke({\n",
    "    \"question\": question,\n",
    "    \"task_id\": \"1\",\n",
    "    \"file_name\": None,\n",
    "    \"file_content\": None,\n",
    "    \"answer\": \"\",\n",
    "    \"final_answer_reached\": False,\n",
    "    \"messages\": [HumanMessage(content=prompt)],\n",
    "})\n",
    "\n",
    "print(task_answer[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37f0dad",
   "metadata": {},
   "source": [
    "# Test notebook\n",
    "- Instructions: https://huggingface.co/learn/agents-course/unit4/hands-on\n",
    "- Leaderboard and codes: https://huggingface.co/spaces/agents-course/Students_leaderboard\n",
    "    - 1 good example: https://huggingface.co/spaces/cellerson/AgentCourseFinalProject/tree/main\n",
    "- GitHub models: https://github.com/marketplace?type=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd8c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ee390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaros\\Documents\\Python\\hf_agents_course_final\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import inspect\n",
    "import typing\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_url = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "questions_url = f\"{api_url}/questions\"\n",
    "random_question_url = f\"{api_url}/random-question\"\n",
    "files_url = f\"{api_url}/files\"\n",
    "submit_url = f\"{api_url}/submit\"\n",
    "\n",
    "def get_question() -> typing.Tuple[str, str, bytes | None, str | None]:\n",
    "    \"\"\"\n",
    "    Get a random question from the API.\n",
    "    Returns\n",
    "        - question text\n",
    "        - task id\n",
    "        - binary file (if any)\n",
    "        - file name (if any)\n",
    "    \"\"\"\n",
    "\n",
    "    question_data = requests.get(questions_url, timeout=15)\n",
    "    question_data.raise_for_status()\n",
    "    questions_json = question_data.json()[0]\n",
    "\n",
    "    question = questions_json[\"question\"]\n",
    "    task_id = questions_json[\"task_id\"]\n",
    "    file_content = None\n",
    "\n",
    "    file_name = questions_json.get(\"file_name\", None)\n",
    "\n",
    "    if file_name:\n",
    "        file_url = f\"{files_url}/{task_id}\"\n",
    "        response = requests.get(file_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        # with open(os.path.join(\"tmp\", file_name), \"wb\") as f:\n",
    "        #     f.write(response.content)\n",
    "        file_content = response.content\n",
    "\n",
    "    return question, task_id, file_content, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2c84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
      "Response:  Between 2000 and 2009, Mercedes Sosa released eight studio albums. Here is a list of those albums:\n",
      "\n",
      "1. **Mujeres Argentinas** (2000)\n",
      "2. **Cantora** (2001)\n",
      "3. **30 Años de Vida de Mercedes Sosa** (2004)\n",
      "4. **Y cómo es él** (2005)\n",
      "5. **Canta por los ausentes** (2006) – Although primarily distributed as a promotion album, it has significant content.\n",
      "6. **Alta fidelidad** (2007)\n",
      "7. **Rodolfo** (2008) – A collaboration with Joan Manuel Serrat.\n",
      "8. **Cantora 2** (2009)\n",
      "\n",
      "Please note that the released albums can vary in definition based on official releases versus compilations or promotional distributions, but these are the mainstream studio albums released within that time frame.\n"
     ]
    }
   ],
   "source": [
    "question, task_id, file_content, file_name = get_question()\n",
    "\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = \"https://models.github.ai/inference\"\n",
    "model_name = \"microsoft/Phi-4\"\n",
    "token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(token),\n",
    ")\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        UserMessage(question),\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1000,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c639857",
   "metadata": {},
   "source": [
    "# Agent\n",
    "## Tools:\n",
    "- Text from picture\n",
    "- Text from CSV\n",
    "- Text from XLSX\n",
    "## Ideas:\n",
    "- Add another agent that will validate the final answer\n",
    "## Issues:\n",
    "- Getting stuck on infinite loop with web search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f45e6",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b3a5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from typing import TypedDict, List, Dict, Annotated, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langchain_community.tools import Tool, DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f846e0e",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba73e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text(query: str) -> str:\n",
    "#     \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "#     results = bm25_retriever.invoke(query)\n",
    "#     if results:\n",
    "#         return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "#     else:\n",
    "#         return \"No matching guest information found.\"\n",
    "\n",
    "# guest_info_tool = Tool(\n",
    "#     name=\"guest_info_retriever\",\n",
    "#     func=extract_text,\n",
    "#     description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    "# )\n",
    "\n",
    "### Search tool\n",
    "\n",
    "# search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "search_wrapper = GoogleSerperAPIWrapper()\n",
    "\n",
    "search_tool = Tool(\n",
    "        name=\"search_tool\",\n",
    "        func=search_wrapper.run,\n",
    "        description=\"useful for when you need to ask with search\",\n",
    "    )\n",
    "\n",
    "### Python tool\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "python_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    search_tool,\n",
    "    python_tool,\n",
    "]\n",
    "\n",
    "model_with_tools = model.bind_tools(tools, parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e6137",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef7b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskState(TypedDict):\n",
    "\n",
    "    # Input data\n",
    "    question: str\n",
    "    task_id: str\n",
    "    file_name: Optional[str]\n",
    "    file_content: Optional[bytes]\n",
    "\n",
    "    # Output data\n",
    "    answer: str\n",
    "    final_answer_reached: bool\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "def assistant(state: TaskState):\n",
    "\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"final_answer_reached\": True,\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3aec6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "task_graph = StateGraph(TaskState)\n",
    "\n",
    "# Add nodes\n",
    "task_graph.add_node(\"assistant\", assistant)\n",
    "task_graph.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Add edges\n",
    "task_graph.add_edge(START, \"assistant\")\n",
    "task_graph.add_conditional_edges(\"assistant\", tools_condition)\n",
    "task_graph.add_edge(\"tools\", \"assistant\")\n",
    "# task_graph.add_edge(\"assistant\", END)\n",
    "\n",
    "compiled_graph = task_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b92fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n"
     ]
    }
   ],
   "source": [
    "question, task_id, file_content, file_name = get_question()\n",
    "\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d30664",
   "metadata": {},
   "outputs": [
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 5 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mHow many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mYou are an AI assistant anwering complex questions.\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mThink step by step and answer following question.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m task_answer = \u001b[43mcompiled_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile_content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinal_answer_reached\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(task_answer[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaros\\Documents\\Python\\hf_agents_course_final\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2795\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2793\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2794\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2795\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2796\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2799\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2800\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2801\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2804\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2805\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2806\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaros\\Documents\\Python\\hf_agents_course_final\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2453\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2445\u001b[39m     msg = create_error_message(\n\u001b[32m   2446\u001b[39m         message=(\n\u001b[32m   2447\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2452\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2453\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2454\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2455\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 5 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "question = \"How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI assistant anwering complex questions.\n",
    "Think step by step and answer following question.\n",
    "Be very concise and output only the answer.\n",
    "Answer has to be formatted as specified in the question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "task_answer = compiled_graph.invoke(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"task_id\": \"1\",\n",
    "        \"file_name\": None,\n",
    "        \"file_content\": None,\n",
    "        \"answer\": \"\",\n",
    "        \"final_answer_reached\": False,\n",
    "        \"messages\": [HumanMessage(content=prompt)],\n",
    "    },\n",
    "    {\"recursion_limit\": 5},\n",
    ")\n",
    "\n",
    "print(task_answer[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
